{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Data/Matsuo Basho's Haikus.json\",'r')\n",
    "data = json.load(file)\n",
    "theData = ''.join(data['Poems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Data/db.json\",'r')\n",
    "Data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'issa': {'author': 'Kobayashi Issa',\n",
       "  'haikus': ['A sudden shower falls -\\nand naked I am riding\\non a naked horse.',\n",
       "   'Summer shower -\\nnaked horse\\na naked rider.',\n",
       "   'A frog and I,\\neyeball to eyeball.\\nMy empty face,\\nbetrayed by lightening.',\n",
       "   'Cool breeze,\\ntangled\\nin a grass-blade.',\n",
       "   'Step by step\\nup a summer mountain -\\nsuddenly: the sea.',\n",
       "   'Cries of wild geese,\\nrumors spread about me.',\n",
       "   'Stillness -\\nclouds peak\\nin the lake.',\n",
       "   \"Just by being,\\nI'm here -\\nin the snow-fall.\",\n",
       "   'Showering\\nonto Mount Kiso,\\nthe Milky Way.',\n",
       "   'What a moon -\\nif only my grumbling wife\\nwere here.',\n",
       "   'In this windy nest\\nopen your hungry\\nmouth in vain... \\nIssa, stepchild bird',\n",
       "   'On the death of his child:\\nDew evaporates\\nand all our world\\nis dew...  So dear,\\nSo fresh, so fleeting',\n",
       "   'A gate made all of twigs\\nWith woven grass\\nFor hinges... \\nFor a lock...  This snail',\n",
       "   'Arise from sleep, old cat,\\nand with great yawns\\nand stretchings... \\nAmble out for love',\n",
       "   'Hi! My little hut\\nis newly-thatched\\nI see... \\nBlue morning-glories',\n",
       "   'Dim the grey cow comes\\nmooing mooing\\nand mooing\\nOut of the morning mist',\n",
       "   'What a peony... \\ndemanding to be\\nmeasured\\nBy my little fan!',\n",
       "   'A nursemaid scarecrow... \\nfrightening the\\nwind and sun\\nFrom playing baby',\n",
       "   'A saddening world:\\nflowers whose sweet\\nblooms must fall... \\nAs we too, alas... ',\n",
       "   'Hi! Kids mimicking\\ncormorants...  You are\\nmore like\\nReal cormorants than\\nThey!',\n",
       "   'Over the mountain\\nbright the full white\\nmoon now smiles... \\nOn the flower-thief',\n",
       "   'Good friend grasshopper\\nwill you play\\nthe caretaker\\nFor my little grave?',\n",
       "   'Giddy grasshopper\\ntake care...  Do not\\nleap and crush\\nThese pearls of dewdrop',\n",
       "   'Now be a good boy\\ntake good care of\\nour house... \\nCricket my child',\n",
       "   'Good evening breeze!\\ncrooked and\\nmeandering\\nYour homeward journey',\n",
       "   'The turnip farmer rose\\nand with a fresh-\\npulled turnip... \\nPointed to my road',\n",
       "   'I am going out... \\nbe good and play\\ntogether\\nMy cricket children',\n",
       "   'If strangers threaten\\nturn into fat\\ngreen bullfrogs... \\nPond-cooling melons',\n",
       "   'Live in simple faith... \\njust as this\\ntrusting cherry\\nFlowers, fades, and falls',\n",
       "   'Oh do not swat them... \\nunhappy flies\\nforever\\nWringing their thin hands',\n",
       "   'In the city fields\\ncontemplating\\ncherry-trees... \\nStrangers are like friends',\n",
       "   'Yellow autumn moon... \\nunimpressed\\nthe scarecrow stands\\nSimply looking bored',\n",
       "   'Cruel autumn wind\\ncutting to the\\nvery bones... \\nOf my poor scarecrow',\n",
       "   'I must turn over... \\nbeware of local\\nearthquakes\\nBedfellow cricket!',\n",
       "   'Visiting the graves... \\ntrotting on to show\\nthe way... \\nOld family dog',\n",
       "   'Before boiled chestnuts\\ncross-legged lad\\nis squatting... \\nCarved wooden Buddha',\n",
       "   'Nice: wild persimmons... \\nand notice how\\nthe mother\\nEats the bitter parts',\n",
       "   'What a gorgeous one\\nthat fat sleek huge\\nold chestnut\\nI could not get at... ',\n",
       "   'Oh former renter\\nI know it all, all... \\ndown to\\nThe very cold you felt',\n",
       "   'Plume of pampas grass\\ntrembling\\nin every wind... \\nHush, my lonely heart',\n",
       "   'Considerate dogs... \\nstepping off\\ninto the snow\\nAs I walk the path',\n",
       "   'Buddha on the hill... \\nfrom your holy\\nnose indeed\\nHangs an icicle',\n",
       "   'The orphan speaks:\\nthe year-end party... \\nI am even envious\\nOf scolded children']},\n",
       " 'basho': {'author': 'Matsuo Basho',\n",
       "  'haikus': ['None is travelling\\nHere along this way but I,\\nThis autumn evening.',\n",
       "   'The first day of the year:\\nthoughts come - and there is loneliness;\\nthe autumn dusk is here.',\n",
       "   'An old pond\\nA frog jumps in -\\nSplash!',\n",
       "   'Old dark sleepy pool... \\nquick unexpected\\nfrog\\nGoes plop! Watersplash!',\n",
       "   \"Lightening -\\nHeron's cry\\nStabs the darkness\",\n",
       "   'Clouds come from time to time -\\nand bring to men a chance to rest\\nfrom looking at the moon.',\n",
       "   \"In the cicada's cry\\nThere's no sign that can foretell\\nHow soon it must die.\",\n",
       "   \"Poverty's child -\\nhe starts to grind the rice,\\nand gazes at the moon.\",\n",
       "   \"Won't you come and see\\nloneliness? Just one leaf\\nfrom the kiri tree.\",\n",
       "   'Temple bells die out.\\nThe fragrant blossoms remain.\\nA perfect evening!',\n",
       "   'Ballet in the air ...\\ntwin butterflies\\nuntil, twice white\\nThey meet, they mate',\n",
       "   'Black cloudbank broken\\nscatters in the\\nnight ... Now see\\nMoon-lighted mountains!',\n",
       "   'Seek on high bare trails\\nsky-reflecting\\nviolets...\\nMountain-top jewels',\n",
       "   'For a lovely bowl\\nlet us arrange these\\nflowers...\\nSince there is no rice',\n",
       "   'Now that eyes of hawks\\nin dusky night\\nare darkened... \\nChirping of the quails',\n",
       "   \"April's air stirs in\\nwillow-leaves... \\na butterfly\\nFloats and balances\",\n",
       "   'In the sea-surf edge\\nmingling with\\nbright small shells ..\\nBush-clover petals',\n",
       "   'The river\\nGathering may rains\\nfrom cold streamlets\\nfor the sea... \\nMurmuring Mogami',\n",
       "   'White cloud of mist\\nabove white\\ncherry-blossoms... \\nDawn-shining mountains',\n",
       "   'Twilight whippoorwill... \\nwhistle on,\\nsweet deepener\\nOf dark loneliness',\n",
       "   'Mountain-rose petals\\nfalling, falling,\\nfalling now... \\nWaterfall music',\n",
       "   'Ah me! I am one\\nwho spends his little\\nbreakfast\\nMorning-glory gazing',\n",
       "   'Seas are wild tonight... \\nstretching over\\nSado Island\\nSilent clouds of stars',\n",
       "   'Why so scrawny, cat?\\nstarving for fat fish\\nor mice... \\nOr backyard love?',\n",
       "   'Dewdrop, let me cleanse\\nin your brief\\nsweet waters... \\nThese dark hands of life',\n",
       "   'Glorious the moon... \\ntherefore our thanks\\ndark clouds\\nCome to rest our necks',\n",
       "   'Under cherry-trees\\nsoup, the salad,\\nfish and all... \\nSeasoned with petals',\n",
       "   'Too curious flower\\nwatching us pass,\\nmet death... \\nOur hungry donkey',\n",
       "   'Cloud of cherry-bloom... \\ntolling twilight\\nbell...  Temple\\nUeno? Asakura?',\n",
       "   'Must springtime fade?\\nthen cry all birds... \\nand fishes\\nCold pale eyes pour tears',\n",
       "   \"Such utter silence!\\neven the crickets'\\nsinging... \\nMuffled by hot rocks\",\n",
       "   'Swallow in the dusk... \\nspare my little\\nbuzzing friends\\nAmong the flowers',\n",
       "   'Reply:\\nBright red pepper-pod... \\nit needs but shiny\\nwings and look... \\nDarting dragon-fly!',\n",
       "   'Wake! The sky is light!\\nlet us to the road\\nagain... \\nCompanion butterfly!',\n",
       "   'Silent the old town... \\nthe scent of flowers\\nfloating... \\nAnd evening bell',\n",
       "   'Camellia-petal\\nfell in silent dawn... \\nspilling\\nA water-jewel',\n",
       "   'In the twilight rain\\nthese brilliant-hued\\nhibiscus... \\nA lovely sunset',\n",
       "   'Lady butterfly\\nperfumes her wings\\nby floating\\nOver the orchid',\n",
       "   'Now the swinging bridge\\nis quieted\\nwith creepers... \\nLike our tendrilled life',\n",
       "   'The sea darkening... \\noh voices of the\\nwild ducks\\nCrying, whirling, white',\n",
       "   'Nine times arising\\nto see the moon... \\nwhose solemn pace\\nMarks only midnight yet',\n",
       "   'Here, where a thousand\\ncaptains swore grand\\nconquest...  Tall\\nGrass their monument',\n",
       "   'Now in sad autumn\\nas I take my\\ndarkening path... \\nA solitary bird',\n",
       "   'Will we meet again\\nhere at your\\nflowering grave... \\nTwo white butterflies?',\n",
       "   'Dry cheerful cricket\\nchirping, keeps\\nthe autumn gay... \\nContemptuous of frost',\n",
       "   'First white snow of fall\\njust enough to bend\\nthe leaves\\nOf faded daffodils',\n",
       "   'Carven gods long gone... \\ndead leaves alone\\nforegather\\nOn the temple porch',\n",
       "   'Cold first winter rain... \\npoor monkey,\\nyou too could use\\nA little woven cape',\n",
       "   'No oil to read by... \\nI am off to bed\\nbut ah!... \\nMy moonlit pillow',\n",
       "   \"This snowy morning\\nthat black crow\\nI hate so much... \\nBut he's beautiful!\",\n",
       "   'If there were fragrance\\nthese heavy snow-\\nflakes settling... \\nLilies on the rocks',\n",
       "   'See: surviving suns\\nvisit the ancestral\\ngrave... \\nBearded, with bent canes',\n",
       "   'Death-song:\\nFever-felled half-way,\\nmy dreams arose\\nTo march again... \\nInto a hollow land']},\n",
       " 'buson': {'author': 'Yosa Buson',\n",
       "  'haikus': ['Standing still at dusk\\nlisten...  In far\\ndistances\\nThe song of froglings!',\n",
       "   'My two plum trees are\\nso gracious... \\nsee, they flower\\nOne now, one later',\n",
       "   'The laden wagon runs\\nbumbling and creaking\\ndown the road... \\nThree peonies tremble',\n",
       "   'Lightning flash, crash... \\nwaiting in the\\nbamboo grove\\nSee three dew-drops fall',\n",
       "   'Afternoon shower... \\nwalking and talking\\nin the street:\\nUmbrella and raincoat!',\n",
       "   'Sadness at twilight... \\nvillain! I have\\nlet my hand\\nCut that peony',\n",
       "   'In dim dusk and scent\\na witness\\nnow half hidden... \\nEvenfall orchid',\n",
       "   'Voices of two bells\\nthat speak from\\ntwilight temples... \\nAh! Cool dialogue',\n",
       "   \"Deep in dark forest\\na woodcutter's\\ndull axe talking... \\nAnd a woodcutter\",\n",
       "   'Butterfly asleep\\nfolded soft on\\ntemple bell... \\nThen bronze gong rang!',\n",
       "   'See the morning breeze\\nruffling his so\\nsilky hair... \\nCool caterpillar',\n",
       "   'A camellia\\ndropped down into\\nstill waters\\nOf a deep dark well',\n",
       "   'In the holy dusk\\nnightingales begin\\ntheir psalm... \\nGood! The dinner-gong!',\n",
       "   'A short summer night... \\nbut in this solemn\\ndarkness\\nOne peony bloomed',\n",
       "   'Pebbles shining clear,\\nand clear\\nsix silent fishes... \\nDeep autumn water',\n",
       "   'A bright autumn moon... \\nin the shadow of\\neach grass\\nAn insect chirping',\n",
       "   'White chrysanthemum... \\nbefore that\\nperfect flower\\nScissors hesitate',\n",
       "   'At furue in rain\\ngray water and\\ngrey sand... \\nPicture without lines',\n",
       "   'The old fisherman\\nunalterably\\nintent... \\nCold evening rain',\n",
       "   \"Rainy-month, dripping\\non and on\\nas I lie abed... \\nAh, old man's memories!\",\n",
       "   'Slanting lines of rain... \\non the dusty\\nsamisen\\nA mouse is trotting',\n",
       "   'Old weary willows... \\nI thought how long\\nthe road would be\\nWhen you went away']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theData = theData.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(theData)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "datCoded = ([ vocab.index(ch) for ch in theData  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 10\n",
    "xDat = []\n",
    "yDat = []\n",
    "for i in range(0,len(theData) - order):\n",
    "    xPt = datCoded[i:i+order]\n",
    "    yPt = datCoded[i+order]\n",
    "    xDat.extend([xPt])\n",
    "    yDat.extend([yPt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdat = np.reshape(xDat,(np.shape(xDat)[0], np.shape(xDat)[1], 1))/len(vocab)\n",
    "Ydat = np_utils.to_categorical(yDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(Xdat.shape[1], Xdat.shape[2])))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(Ydat.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 35)                3535      \n",
      "=================================================================\n",
      "Total params: 44,335\n",
      "Trainable params: 44,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6660 samples, validate on 1665 samples\n",
      "Epoch 1/200\n",
      "6660/6660 [==============================] - 1s 118us/step - loss: 3.0198 - val_loss: 3.0210\n",
      "Epoch 2/200\n",
      "6660/6660 [==============================] - 1s 126us/step - loss: 3.0200 - val_loss: 3.0190\n",
      "Epoch 3/200\n",
      "6660/6660 [==============================] - 1s 126us/step - loss: 3.0194 - val_loss: 3.0169\n",
      "Epoch 4/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 3.0191 - val_loss: 3.0152\n",
      "Epoch 5/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 3.0159 - val_loss: 3.0136\n",
      "Epoch 6/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 3.0165 - val_loss: 3.0103\n",
      "Epoch 7/200\n",
      "6660/6660 [==============================] - 1s 138us/step - loss: 3.0095 - val_loss: 3.0067\n",
      "Epoch 8/200\n",
      "6660/6660 [==============================] - 1s 124us/step - loss: 3.0099 - val_loss: 3.0024\n",
      "Epoch 9/200\n",
      "6660/6660 [==============================] - 1s 140us/step - loss: 3.0051 - val_loss: 2.9957\n",
      "Epoch 10/200\n",
      "6660/6660 [==============================] - 1s 137us/step - loss: 3.0014 - val_loss: 2.9892\n",
      "Epoch 11/200\n",
      "6660/6660 [==============================] - 1s 127us/step - loss: 2.9935 - val_loss: 2.9844\n",
      "Epoch 12/200\n",
      "6660/6660 [==============================] - 1s 111us/step - loss: 2.9931 - val_loss: 2.9780\n",
      "Epoch 13/200\n",
      "6660/6660 [==============================] - 1s 140us/step - loss: 2.9870 - val_loss: 2.9745\n",
      "Epoch 14/200\n",
      "6660/6660 [==============================] - 1s 192us/step - loss: 2.9823 - val_loss: 2.9744\n",
      "Epoch 15/200\n",
      "6660/6660 [==============================] - 1s 146us/step - loss: 2.9865 - val_loss: 2.9646\n",
      "Epoch 16/200\n",
      "6660/6660 [==============================] - 1s 192us/step - loss: 2.9750 - val_loss: 2.9631\n",
      "Epoch 17/200\n",
      "6660/6660 [==============================] - 1s 185us/step - loss: 2.9671 - val_loss: 2.9594\n",
      "Epoch 18/200\n",
      "6660/6660 [==============================] - 1s 158us/step - loss: 2.9558 - val_loss: 2.9491\n",
      "Epoch 19/200\n",
      "6660/6660 [==============================] - 1s 175us/step - loss: 2.9525 - val_loss: 2.9407\n",
      "Epoch 20/200\n",
      "6660/6660 [==============================] - 1s 157us/step - loss: 2.9524 - val_loss: 2.9374\n",
      "Epoch 21/200\n",
      "6660/6660 [==============================] - 1s 167us/step - loss: 2.9480 - val_loss: 2.9349\n",
      "Epoch 22/200\n",
      "6660/6660 [==============================] - 1s 178us/step - loss: 2.9408 - val_loss: 2.9272\n",
      "Epoch 23/200\n",
      "6660/6660 [==============================] - ETA: 0s - loss: 2.943 - 1s 158us/step - loss: 2.9425 - val_loss: 2.9250\n",
      "Epoch 24/200\n",
      "6660/6660 [==============================] - 1s 162us/step - loss: 2.9381 - val_loss: 2.9201\n",
      "Epoch 25/200\n",
      "6660/6660 [==============================] - 1s 149us/step - loss: 2.9299 - val_loss: 2.9181\n",
      "Epoch 26/200\n",
      "6660/6660 [==============================] - 1s 153us/step - loss: 2.9216 - val_loss: 2.9135\n",
      "Epoch 27/200\n",
      "6660/6660 [==============================] - 1s 149us/step - loss: 2.9140 - val_loss: 2.9085\n",
      "Epoch 28/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.9141 - val_loss: 2.9040\n",
      "Epoch 29/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.9146 - val_loss: 2.9023\n",
      "Epoch 30/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.9149 - val_loss: 2.8987\n",
      "Epoch 31/200\n",
      "6660/6660 [==============================] - 1s 136us/step - loss: 2.9111 - val_loss: 2.8967\n",
      "Epoch 32/200\n",
      "6660/6660 [==============================] - 1s 137us/step - loss: 2.9062 - val_loss: 2.8939\n",
      "Epoch 33/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.9022 - val_loss: 2.8951\n",
      "Epoch 34/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.9022 - val_loss: 2.8900\n",
      "Epoch 35/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.8983 - val_loss: 2.8877\n",
      "Epoch 36/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8929 - val_loss: 2.8852\n",
      "Epoch 37/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.8911 - val_loss: 2.8819\n",
      "Epoch 38/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.8941 - val_loss: 2.8814\n",
      "Epoch 39/200\n",
      "6660/6660 [==============================] - 1s 126us/step - loss: 2.8895 - val_loss: 2.8787\n",
      "Epoch 40/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8850 - val_loss: 2.8770\n",
      "Epoch 41/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8821 - val_loss: 2.8733\n",
      "Epoch 42/200\n",
      "6660/6660 [==============================] - 1s 152us/step - loss: 2.8782 - val_loss: 2.8724\n",
      "Epoch 43/200\n",
      "6660/6660 [==============================] - 1s 162us/step - loss: 2.8766 - val_loss: 2.8695\n",
      "Epoch 44/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8731 - val_loss: 2.8677\n",
      "Epoch 45/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.8727 - val_loss: 2.8645\n",
      "Epoch 46/200\n",
      "6660/6660 [==============================] - 1s 140us/step - loss: 2.8676 - val_loss: 2.8627\n",
      "Epoch 47/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.8681 - val_loss: 2.8601\n",
      "Epoch 48/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.8645 - val_loss: 2.8569\n",
      "Epoch 49/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.8641 - val_loss: 2.8569\n",
      "Epoch 50/200\n",
      "6660/6660 [==============================] - 1s 135us/step - loss: 2.8608 - val_loss: 2.8560\n",
      "Epoch 51/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.8572 - val_loss: 2.8550\n",
      "Epoch 52/200\n",
      "6660/6660 [==============================] - 1s 134us/step - loss: 2.8543 - val_loss: 2.8535\n",
      "Epoch 53/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.8505 - val_loss: 2.8499\n",
      "Epoch 54/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.8536 - val_loss: 2.8457\n",
      "Epoch 55/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.8474 - val_loss: 2.8448\n",
      "Epoch 56/200\n",
      "6660/6660 [==============================] - 1s 127us/step - loss: 2.8429 - val_loss: 2.8433\n",
      "Epoch 57/200\n",
      "6660/6660 [==============================] - 1s 140us/step - loss: 2.8434 - val_loss: 2.8422\n",
      "Epoch 58/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.8381 - val_loss: 2.8415\n",
      "Epoch 59/200\n",
      "6660/6660 [==============================] - 1s 128us/step - loss: 2.8385 - val_loss: 2.8444\n",
      "Epoch 60/200\n",
      "6660/6660 [==============================] - 1s 138us/step - loss: 2.8340 - val_loss: 2.8405\n",
      "Epoch 61/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.8332 - val_loss: 2.8392\n",
      "Epoch 62/200\n",
      "6660/6660 [==============================] - 1s 127us/step - loss: 2.8294 - val_loss: 2.8379\n",
      "Epoch 63/200\n",
      "6660/6660 [==============================] - 1s 126us/step - loss: 2.8236 - val_loss: 2.8364\n",
      "Epoch 64/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.8303 - val_loss: 2.8366\n",
      "Epoch 65/200\n",
      "6660/6660 [==============================] - 1s 122us/step - loss: 2.8237 - val_loss: 2.8382\n",
      "Epoch 66/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8292 - val_loss: 2.8395\n",
      "Epoch 67/200\n",
      "6660/6660 [==============================] - 1s 126us/step - loss: 2.8247 - val_loss: 2.8369\n",
      "Epoch 68/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.8194 - val_loss: 2.8354\n",
      "Epoch 69/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.8166 - val_loss: 2.8352\n",
      "Epoch 70/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.8185 - val_loss: 2.8379\n",
      "Epoch 71/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8177 - val_loss: 2.8342\n",
      "Epoch 72/200\n",
      "6660/6660 [==============================] - 1s 136us/step - loss: 2.8114 - val_loss: 2.8333\n",
      "Epoch 73/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.8133 - val_loss: 2.8304\n",
      "Epoch 74/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.8143 - val_loss: 2.8313\n",
      "Epoch 75/200\n",
      "6660/6660 [==============================] - 1s 125us/step - loss: 2.8127 - val_loss: 2.8321\n",
      "Epoch 76/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.8098 - val_loss: 2.8317\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.8003 - val_loss: 2.8307\n",
      "Epoch 78/200\n",
      "6660/6660 [==============================] - 1s 135us/step - loss: 2.8047 - val_loss: 2.8320\n",
      "Epoch 79/200\n",
      "6660/6660 [==============================] - 1s 148us/step - loss: 2.7989 - val_loss: 2.8313\n",
      "Epoch 80/200\n",
      "6660/6660 [==============================] - 1s 157us/step - loss: 2.7959 - val_loss: 2.8327\n",
      "Epoch 81/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 2.7925 - val_loss: 2.8317\n",
      "Epoch 82/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.7984 - val_loss: 2.8310\n",
      "Epoch 83/200\n",
      "6660/6660 [==============================] - 1s 157us/step - loss: 2.7907 - val_loss: 2.8300\n",
      "Epoch 84/200\n",
      "6660/6660 [==============================] - 1s 161us/step - loss: 2.7940 - val_loss: 2.8305\n",
      "Epoch 85/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.7929 - val_loss: 2.8319\n",
      "Epoch 86/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.7891 - val_loss: 2.8293\n",
      "Epoch 87/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 2.7938 - val_loss: 2.8328\n",
      "Epoch 88/200\n",
      "6660/6660 [==============================] - 1s 146us/step - loss: 2.7911 - val_loss: 2.8358\n",
      "Epoch 89/200\n",
      "6660/6660 [==============================] - 1s 136us/step - loss: 2.7863 - val_loss: 2.8449\n",
      "Epoch 90/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.7827 - val_loss: 2.8487\n",
      "Epoch 91/200\n",
      "6660/6660 [==============================] - 1s 137us/step - loss: 2.7885 - val_loss: 2.8434\n",
      "Epoch 92/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.7820 - val_loss: 2.8383\n",
      "Epoch 93/200\n",
      "6660/6660 [==============================] - 1s 127us/step - loss: 2.7781 - val_loss: 2.8336\n",
      "Epoch 94/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.7744 - val_loss: 2.8326\n",
      "Epoch 95/200\n",
      "6660/6660 [==============================] - 1s 131us/step - loss: 2.7716 - val_loss: 2.8316\n",
      "Epoch 96/200\n",
      "6660/6660 [==============================] - 1s 136us/step - loss: 2.7700 - val_loss: 2.8330\n",
      "Epoch 97/200\n",
      "6660/6660 [==============================] - 1s 128us/step - loss: 2.7661 - val_loss: 2.8317\n",
      "Epoch 98/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.7578 - val_loss: 2.8310\n",
      "Epoch 99/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.7616 - val_loss: 2.8375\n",
      "Epoch 100/200\n",
      "6660/6660 [==============================] - 1s 125us/step - loss: 2.7673 - val_loss: 2.8333\n",
      "Epoch 101/200\n",
      "6660/6660 [==============================] - 1s 127us/step - loss: 2.7604 - val_loss: 2.8371\n",
      "Epoch 102/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.7604 - val_loss: 2.8424\n",
      "Epoch 103/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.7558 - val_loss: 2.8450\n",
      "Epoch 104/200\n",
      "6660/6660 [==============================] - 1s 128us/step - loss: 2.7556 - val_loss: 2.8483\n",
      "Epoch 105/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.7549 - val_loss: 2.8452\n",
      "Epoch 106/200\n",
      "6660/6660 [==============================] - 1s 132us/step - loss: 2.7512 - val_loss: 2.8399\n",
      "Epoch 107/200\n",
      "6660/6660 [==============================] - 1s 133us/step - loss: 2.7530 - val_loss: 2.8401\n",
      "Epoch 108/200\n",
      "6660/6660 [==============================] - 1s 129us/step - loss: 2.7502 - val_loss: 2.8378\n",
      "Epoch 109/200\n",
      "6660/6660 [==============================] - 1s 127us/step - loss: 2.7404 - val_loss: 2.8390\n",
      "Epoch 110/200\n",
      "6660/6660 [==============================] - 1s 136us/step - loss: 2.7431 - val_loss: 2.8371\n",
      "Epoch 111/200\n",
      "6660/6660 [==============================] - 1s 130us/step - loss: 2.7385 - val_loss: 2.8323\n",
      "Epoch 112/200\n",
      "6660/6660 [==============================] - 1s 138us/step - loss: 2.7431 - val_loss: 2.8332\n",
      "Epoch 113/200\n",
      "6660/6660 [==============================] - 1s 159us/step - loss: 2.7347 - val_loss: 2.8335\n",
      "Epoch 114/200\n",
      "6660/6660 [==============================] - 1s 161us/step - loss: 2.7409 - val_loss: 2.8453\n",
      "Epoch 115/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 2.7425 - val_loss: 2.8619\n",
      "Epoch 116/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 2.7359 - val_loss: 2.8565\n",
      "Epoch 117/200\n",
      "6660/6660 [==============================] - 1s 137us/step - loss: 2.7319 - val_loss: 2.8472\n",
      "Epoch 118/200\n",
      "6660/6660 [==============================] - 1s 141us/step - loss: 2.7238 - val_loss: 2.8500\n",
      "Epoch 119/200\n",
      "6660/6660 [==============================] - 1s 150us/step - loss: 2.7253 - val_loss: 2.8541\n",
      "Epoch 120/200\n",
      "6660/6660 [==============================] - 1s 187us/step - loss: 2.7233 - val_loss: 2.8549\n",
      "Epoch 121/200\n",
      "6660/6660 [==============================] - 1s 155us/step - loss: 2.7175 - val_loss: 2.8507\n",
      "Epoch 122/200\n",
      "6660/6660 [==============================] - 1s 155us/step - loss: 2.7084 - val_loss: 2.8566\n",
      "Epoch 123/200\n",
      "6660/6660 [==============================] - 1s 163us/step - loss: 2.7080 - val_loss: 2.8545\n",
      "Epoch 124/200\n",
      "6660/6660 [==============================] - 1s 172us/step - loss: 2.7130 - val_loss: 2.8586\n",
      "Epoch 125/200\n",
      "6660/6660 [==============================] - 1s 158us/step - loss: 2.7123 - val_loss: 2.8633\n",
      "Epoch 126/200\n",
      "6660/6660 [==============================] - 1s 161us/step - loss: 2.7068 - val_loss: 2.8633\n",
      "Epoch 127/200\n",
      "6660/6660 [==============================] - 1s 145us/step - loss: 2.7031 - val_loss: 2.8568\n",
      "Epoch 128/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.7021 - val_loss: 2.8670\n",
      "Epoch 129/200\n",
      "6660/6660 [==============================] - 1s 140us/step - loss: 2.7009 - val_loss: 2.8673\n",
      "Epoch 130/200\n",
      "6660/6660 [==============================] - 1s 154us/step - loss: 2.6927 - val_loss: 2.8624\n",
      "Epoch 131/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.6958 - val_loss: 2.8584\n",
      "Epoch 132/200\n",
      "6660/6660 [==============================] - 1s 142us/step - loss: 2.6900 - val_loss: 2.8592\n",
      "Epoch 133/200\n",
      "6660/6660 [==============================] - 1s 142us/step - loss: 2.6825 - val_loss: 2.8623\n",
      "Epoch 134/200\n",
      "6660/6660 [==============================] - 1s 142us/step - loss: 2.6878 - val_loss: 2.8614\n",
      "Epoch 135/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 2.6840 - val_loss: 2.8593\n",
      "Epoch 136/200\n",
      "6660/6660 [==============================] - 1s 139us/step - loss: 2.6806 - val_loss: 2.8677\n",
      "Epoch 137/200\n",
      "6660/6660 [==============================] - 1s 143us/step - loss: 2.6769 - val_loss: 2.8723\n",
      "Epoch 138/200\n",
      "6660/6660 [==============================] - 1s 152us/step - loss: 2.6797 - val_loss: 2.8689\n",
      "Epoch 139/200\n",
      "6660/6660 [==============================] - 1s 143us/step - loss: 2.6696 - val_loss: 2.8688\n",
      "Epoch 140/200\n",
      "6660/6660 [==============================] - 1s 143us/step - loss: 2.6725 - val_loss: 2.8618\n",
      "Epoch 141/200\n",
      "6660/6660 [==============================] - 1s 145us/step - loss: 2.6669 - val_loss: 2.8698\n",
      "Epoch 142/200\n",
      "6660/6660 [==============================] - 1s 153us/step - loss: 2.6652 - val_loss: 2.8703\n",
      "Epoch 143/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.6568 - val_loss: 2.8689\n",
      "Epoch 144/200\n",
      "6660/6660 [==============================] - 1s 156us/step - loss: 2.6612 - val_loss: 2.8710\n",
      "Epoch 145/200\n",
      "6660/6660 [==============================] - 1s 136us/step - loss: 2.6538 - val_loss: 2.8771\n",
      "Epoch 146/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.6598 - val_loss: 2.8716\n",
      "Epoch 147/200\n",
      "6660/6660 [==============================] - 1s 160us/step - loss: 2.6533 - val_loss: 2.8764\n",
      "Epoch 148/200\n",
      "6660/6660 [==============================] - 1s 165us/step - loss: 2.6460 - val_loss: 2.8774\n",
      "Epoch 149/200\n",
      "6660/6660 [==============================] - 1s 161us/step - loss: 2.6465 - val_loss: 2.8828\n",
      "Epoch 150/200\n",
      "6660/6660 [==============================] - 1s 152us/step - loss: 2.6443 - val_loss: 2.8755\n",
      "Epoch 151/200\n",
      "6660/6660 [==============================] - 1s 137us/step - loss: 2.6454 - val_loss: 2.8829\n",
      "Epoch 152/200\n",
      "6660/6660 [==============================] - 1s 141us/step - loss: 2.6423 - val_loss: 2.8772\n",
      "Epoch 153/200\n",
      "6660/6660 [==============================] - 1s 143us/step - loss: 2.6338 - val_loss: 2.8850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "6660/6660 [==============================] - 1s 143us/step - loss: 2.6389 - val_loss: 2.8847\n",
      "Epoch 155/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.6298 - val_loss: 2.8852\n",
      "Epoch 156/200\n",
      "6660/6660 [==============================] - 1s 149us/step - loss: 2.6339 - val_loss: 2.8786\n",
      "Epoch 157/200\n",
      "6660/6660 [==============================] - 1s 156us/step - loss: 2.6216 - val_loss: 2.8988\n",
      "Epoch 158/200\n",
      "6660/6660 [==============================] - 1s 151us/step - loss: 2.6221 - val_loss: 2.8869\n",
      "Epoch 159/200\n",
      "6660/6660 [==============================] - 1s 158us/step - loss: 2.6224 - val_loss: 2.8910\n",
      "Epoch 160/200\n",
      "6660/6660 [==============================] - 1s 156us/step - loss: 2.6262 - val_loss: 2.8895\n",
      "Epoch 161/200\n",
      "6660/6660 [==============================] - 1s 153us/step - loss: 2.6141 - val_loss: 2.8885\n",
      "Epoch 162/200\n",
      "6660/6660 [==============================] - 1s 155us/step - loss: 2.6123 - val_loss: 2.8900\n",
      "Epoch 163/200\n",
      "6660/6660 [==============================] - 1s 150us/step - loss: 2.6076 - val_loss: 2.8912\n",
      "Epoch 164/200\n",
      "6660/6660 [==============================] - 1s 169us/step - loss: 2.6050 - val_loss: 2.8997\n",
      "Epoch 165/200\n",
      "6660/6660 [==============================] - 1s 149us/step - loss: 2.6085 - val_loss: 2.8853\n",
      "Epoch 166/200\n",
      "6660/6660 [==============================] - 1s 154us/step - loss: 2.6104 - val_loss: 2.8995\n",
      "Epoch 167/200\n",
      "6660/6660 [==============================] - 1s 152us/step - loss: 2.5970 - val_loss: 2.8871\n",
      "Epoch 168/200\n",
      "6660/6660 [==============================] - 1s 153us/step - loss: 2.5984 - val_loss: 2.9001\n",
      "Epoch 169/200\n",
      "6660/6660 [==============================] - 1s 154us/step - loss: 2.5922 - val_loss: 2.8919\n",
      "Epoch 170/200\n",
      "6660/6660 [==============================] - 1s 155us/step - loss: 2.5899 - val_loss: 2.8962\n",
      "Epoch 171/200\n",
      "6660/6660 [==============================] - 1s 142us/step - loss: 2.5801 - val_loss: 2.8925\n",
      "Epoch 172/200\n",
      "6660/6660 [==============================] - 1s 150us/step - loss: 2.5923 - val_loss: 2.8985\n",
      "Epoch 173/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.5805 - val_loss: 2.9024\n",
      "Epoch 174/200\n",
      "6660/6660 [==============================] - 1s 161us/step - loss: 2.5825 - val_loss: 2.8977\n",
      "Epoch 175/200\n",
      "6660/6660 [==============================] - 1s 157us/step - loss: 2.5785 - val_loss: 2.9020\n",
      "Epoch 176/200\n",
      "6660/6660 [==============================] - 1s 153us/step - loss: 2.5690 - val_loss: 2.9006\n",
      "Epoch 177/200\n",
      "6660/6660 [==============================] - 1s 152us/step - loss: 2.5676 - val_loss: 2.9037\n",
      "Epoch 178/200\n",
      "6660/6660 [==============================] - 1s 154us/step - loss: 2.5639 - val_loss: 2.9009\n",
      "Epoch 179/200\n",
      "6660/6660 [==============================] - 1s 159us/step - loss: 2.5653 - val_loss: 2.9105\n",
      "Epoch 180/200\n",
      "6660/6660 [==============================] - 1s 158us/step - loss: 2.5619 - val_loss: 2.9058\n",
      "Epoch 181/200\n",
      "6660/6660 [==============================] - 1s 168us/step - loss: 2.5551 - val_loss: 2.9018\n",
      "Epoch 182/200\n",
      "6660/6660 [==============================] - 1s 158us/step - loss: 2.5484 - val_loss: 2.9151\n",
      "Epoch 183/200\n",
      "6660/6660 [==============================] - 1s 158us/step - loss: 2.5578 - val_loss: 2.9103\n",
      "Epoch 184/200\n",
      "6660/6660 [==============================] - 1s 166us/step - loss: 2.5499 - val_loss: 2.9172\n",
      "Epoch 185/200\n",
      "6660/6660 [==============================] - 1s 147us/step - loss: 2.5514 - val_loss: 2.9129\n",
      "Epoch 186/200\n",
      "6660/6660 [==============================] - 1s 146us/step - loss: 2.5421 - val_loss: 2.9211\n",
      "Epoch 187/200\n",
      "6660/6660 [==============================] - 1s 160us/step - loss: 2.5315 - val_loss: 2.9099\n",
      "Epoch 188/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.5382 - val_loss: 2.9204\n",
      "Epoch 189/200\n",
      "6660/6660 [==============================] - 1s 161us/step - loss: 2.5330 - val_loss: 2.9178\n",
      "Epoch 190/200\n",
      "6660/6660 [==============================] - 1s 154us/step - loss: 2.5309 - val_loss: 2.9175\n",
      "Epoch 191/200\n",
      "6660/6660 [==============================] - 1s 140us/step - loss: 2.5208 - val_loss: 2.9206\n",
      "Epoch 192/200\n",
      "6660/6660 [==============================] - 1s 138us/step - loss: 2.5186 - val_loss: 2.9351\n",
      "Epoch 193/200\n",
      "6660/6660 [==============================] - 1s 146us/step - loss: 2.5173 - val_loss: 2.9244\n",
      "Epoch 194/200\n",
      "6660/6660 [==============================] - 1s 137us/step - loss: 2.5168 - val_loss: 2.9252\n",
      "Epoch 195/200\n",
      "6660/6660 [==============================] - 1s 142us/step - loss: 2.5094 - val_loss: 2.9243\n",
      "Epoch 196/200\n",
      "6660/6660 [==============================] - 1s 144us/step - loss: 2.5123 - val_loss: 2.9297\n",
      "Epoch 197/200\n",
      "6660/6660 [==============================] - 1s 159us/step - loss: 2.5058 - val_loss: 2.9328\n",
      "Epoch 198/200\n",
      "6660/6660 [==============================] - 1s 143us/step - loss: 2.5056 - val_loss: 2.9266\n",
      "Epoch 199/200\n",
      "4200/6660 [=================>............] - ETA: 0s - loss: 2.541 - ETA: 0s - loss: 2.5383"
     ]
    }
   ],
   "source": [
    "history = model.fit(Xdat, Ydat, \n",
    "          epochs=200, batch_size=600,\n",
    "          #callbacks=callbacks_list, \n",
    "          validation_split=0.2,\n",
    "          shuffle=False,\n",
    "          verbose =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.title('Prediction loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "151px",
    "width": "194px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
